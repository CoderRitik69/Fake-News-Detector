# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlC9eVTteFd0xyTDw2SeVAe8ZVoiVRAH
"""

from IPython import get_ipython
from IPython.display import display

import os
os.kill(os.getpid(), 9)

!pip install huggingface_hub

from huggingface_hub import login
login()  # Paste your token from https://huggingface.co/settings/tokens

from google.colab import drive
drive.mount('/content/drive')

!pip install numpy<2.0
!pip install --upgrade datasets

import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from huggingface_hub import HfApi, HfFolder, Repository, notebook_login
from huggingface_hub import login
login(token="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")

def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    return text
# Load your dataset (adjust the path accordingly)
df = pd.read_csv("/content/drive/MyDrive/WELFake_Dataset.csv")
df["text"] = df["text"].astype(str).apply(clean_text)
df = df.rename(columns={"text": "text", "label": "label"})



# Train/test split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["label"])
dataset = Dataset.from_pandas(train_df)
dataset_test = Dataset.from_pandas(test_df)


# Tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")


def tokenize_fn(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=256)


dataset = dataset.map(tokenize_fn, batched=True)
dataset_test = dataset_test.map(tokenize_fn, batched=True)
dataset = dataset.rename_column("label", "labels")
dataset_test = dataset_test.rename_column("label", "labels")


# Explicitly convert columns to numpy arrays before setting torch format
# This might help avoid the copy=False issue later
#for col in ["input_ids", "attention_mask", "labels"]:
#   dataset = dataset.map(lambda examples: {col: np.array(examples[col])}, batched=True)
#   dataset_test = dataset_test.map(lambda examples: {col: np.array(examples[col])}, batched=True)

# Explicitly convert numpy arrays to PyTorch tensors
def to_torch_tensors(examples):
  # Ensure the data is a numpy array before converting to tensor
    # This might already be handled by the previous mapping, but doing it explicitly again
    # ensures we are working with numpy arrays.
    input_ids = np.array(examples["input_ids"])
    attention_mask = np.array(examples["attention_mask"])
    labels = np.array(examples["labels"])

    examples["input_ids"] = torch.tensor(examples["input_ids"])
    examples["attention_mask"] = torch.tensor(examples["attention_mask"])
    # Ensure labels are long tensors if they represent class indices
    examples["labels"] = torch.tensor(examples["labels"], dtype=torch.long)
    return examples

dataset = dataset.map(to_torch_tensors, batched=True)
dataset_test = dataset_test.map(to_torch_tensors, batched=True)


# Explicitly set copy=True to allow datasets to create a copy if needed
#dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"],copy=True)
#dataset_test.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"],copy=True)


# Set the format to "torch". Since we've already converted to tensors,
# datasets should ideally recognize these as torch tensors.
# We might not need to specify columns here explicitly.
dataset.set_format(type="torch")
dataset_test.set_format(type="torch")



# Load model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    prec, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    return {"accuracy": acc, "precision": prec, "recall": recall, "f1": f1}


# Training arguments
training_args = TrainingArguments(
    output_dir="/content/bert-fake-news-model",
    #evaluation_strategy="epoch",
    #save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    #load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_dir="/content/bert-fake-news-model",
    logging_steps=10
)


# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    eval_dataset=dataset_test,
    compute_metrics=compute_metrics
)
# Train
trainer.train()
# Save model
model = BertForSequenceClassification.from_pretrained("/content/bert-fake-news-model")
tokenizer = BertTokenizerFast.from_pretrained("/content/bert-fake-news-model")

model.push_to_hub("RitikNarayan/bert-fake-news")
tokenizer.push_to_hub("RitikNarayan/bert-fake-news")

# Train
trainer.train()

from transformers import BertForSequenceClassification, BertTokenizerFast

# Load from the last checkpoint
model = BertForSequenceClassification.from_pretrained("/content/bert-fake-news-model/checkpoint-21642")
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")  # Same tokenizer as used in training

# Save final model and tokenizer to a clean folder
model.save_pretrained("/content/bert-fake-news-final")
tokenizer.save_pretrained("/content/bert-fake-news-final")

!ls /content/bert-fake-news-final

model.push_to_hub("RitikNarayan/bert-fake-news")
tokenizer.push_to_hub("RitikNarayan/bert-fake-news")

from transformers import BertTokenizerFast, BertForSequenceClassification
import torch

# Load the tokenizer and model from local directory
model_path = "RitikNarayan/bert-fake-news"

try:
    tokenizer = BertTokenizerFast.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path)
    model.eval()
    print("‚úÖ Model and tokenizer loaded successfully!\n")
except Exception as e:
    print("‚ùå Failed to load model/tokenizer:", e)
    exit()

# Prediction function
def predict_fake_news(text):
    if not text.strip():
        return "‚ö†Ô∏è Please enter a non-empty sentence."

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    prediction = torch.argmax(probs).item()
    confidence = torch.max(probs).item() * 100

    label = "üì∞ Real News" if prediction == 0 else "‚ùå Fake News"
    return f"{label} (Confidence: {confidence:.2f}%)"

# Interactive loop
print("üì¢ Welcome to the Fake News Detector!")
print("Type a news headline or short paragraph and press Enter.")
print("Type 'exit' or press Ctrl+C to quit.\n")

while True:
    try:
        user_input = input("üìù Enter text: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üëã Exiting... Stay aware!")
            break
        result = predict_fake_news(user_input)
        print("üîé Prediction:", result, "\n")
    except KeyboardInterrupt:
        print("\nüëã Exiting... Stay aware!")
        break